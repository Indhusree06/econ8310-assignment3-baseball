{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XxdXegRI_eRr"
      },
      "source": [
        "# Baseball Pitch Detection - Assignment 3\n",
        "\n",
        "1. **Custom data loader** for baseball videos\n",
        "2. **PyTorch neural network** (Faster R-CNN) training\n",
        "3. **Model save/load** functionality for evaluation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pb-__6oE_eRs"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import cv2\n",
        "import torch\n",
        "import torchvision\n",
        "import numpy as np\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import xml.etree.ElementTree as ET\n",
        "from collections import defaultdict\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1r8SUHi3_eRs"
      },
      "source": [
        "## Custom Data Loader\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q9UX3epz_eRs"
      },
      "outputs": [],
      "source": [
        "def video_to_tensor(video_path, resize=(640, 480), frame_skip=1):\n",
        "    \"\"\"Convert video file to PyTorch tensor.\"\"\"\n",
        "    print(f\"Loading: {os.path.basename(video_path)}\")\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    frames = []\n",
        "\n",
        "    while True:\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "        if resize:\n",
        "            frame = cv2.resize(frame, resize)\n",
        "        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "        frames.append(frame)\n",
        "\n",
        "    cap.release()\n",
        "\n",
        "    if not frames:\n",
        "        raise ValueError(f\"No frames in {video_path}\")\n",
        "\n",
        "    frames = np.stack(frames)[::frame_skip]\n",
        "    frames = torch.from_numpy(frames).float().permute(0, 3, 1, 2) / 255.0\n",
        "    print(f\"  -> {frames.shape[0]} frames loaded\\\\n\")\n",
        "    return frames\n",
        "\n",
        "\n",
        "def parse_cvat_xml(xml_path, frame_skip=1):\n",
        "    \"\"\"Parse CVAT XML annotation file.\"\"\"\n",
        "    tree = ET.parse(xml_path)\n",
        "    root = tree.getroot()\n",
        "    annotations = defaultdict(list)\n",
        "\n",
        "    for track in root.findall(\"track\"):\n",
        "        label = track.attrib[\"label\"]\n",
        "        for box in track.findall(\"box\"):\n",
        "            frame = int(box.attrib[\"frame\"])\n",
        "            outside = int(box.attrib[\"outside\"])\n",
        "\n",
        "            if outside != 0:\n",
        "                continue\n",
        "\n",
        "            xtl = float(box.attrib[\"xtl\"])\n",
        "            ytl = float(box.attrib[\"ytl\"])\n",
        "            xbr = float(box.attrib[\"xbr\"])\n",
        "            ybr = float(box.attrib[\"ybr\"])\n",
        "\n",
        "            moving_attr = box.find(\"attribute[@name='moving']\")\n",
        "            if moving_attr is None:\n",
        "                continue\n",
        "\n",
        "            moving_flag = 1 if moving_attr.text.lower() == \"true\" else 0\n",
        "\n",
        "            if frame % frame_skip == 0:\n",
        "                adjusted_frame = frame // frame_skip\n",
        "                annotations[adjusted_frame].append({\n",
        "                    \"label\": label,\n",
        "                    \"bbox\": [xtl, ytl, xbr, ybr],\n",
        "                    \"moving\": moving_flag\n",
        "                })\n",
        "\n",
        "    return annotations\n",
        "\n",
        "\n",
        "class BaseballVideoDataset(Dataset):\n",
        "    \"\"\"Custom Dataset for baseball pitch videos with annotations.\"\"\"\n",
        "\n",
        "    def __init__(self, video_dir, xml_dir, resize=(640, 480), frame_skip=1):\n",
        "        self.video_dir = video_dir\n",
        "        self.xml_dir = xml_dir\n",
        "        self.resize = resize\n",
        "        self.frame_skip = frame_skip\n",
        "        self.video_tensors = {}\n",
        "        self.index_map = []\n",
        "\n",
        "        video_files = [f for f in os.listdir(video_dir)\n",
        "                      if f.lower().endswith(('.mp4', '.mov', '.avi'))]\n",
        "\n",
        "        print(f\"Found {len(video_files)} video files\\\\n\")\n",
        "\n",
        "        for video_file in video_files:\n",
        "            stem = os.path.splitext(video_file)[0]\n",
        "            video_path = os.path.join(video_dir, video_file)\n",
        "            xml_path = os.path.join(xml_dir, f\"{stem}.xml\")\n",
        "\n",
        "            if not os.path.exists(xml_path):\n",
        "                print(f\"Skipping {video_file}: no XML\\\\n\")\n",
        "                continue\n",
        "\n",
        "            try:\n",
        "                video_tensor = video_to_tensor(video_path, resize=resize, frame_skip=frame_skip)\n",
        "                annotations = parse_cvat_xml(xml_path, frame_skip=frame_skip)\n",
        "                self.video_tensors[video_path] = video_tensor\n",
        "\n",
        "                for frame_idx, ann_list in annotations.items():\n",
        "                    if len(ann_list) == 0 or frame_idx >= len(video_tensor):\n",
        "                        continue\n",
        "\n",
        "                    boxes = torch.tensor([a[\"bbox\"] for a in ann_list], dtype=torch.float32)\n",
        "                    moving = torch.tensor([a[\"moving\"] for a in ann_list], dtype=torch.int64)\n",
        "\n",
        "                    self.index_map.append((video_path, frame_idx, {\n",
        "                        \"boxes\": boxes,\n",
        "                        \"moving\": moving\n",
        "                    }))\n",
        "\n",
        "                print(f\"Indexed {len(annotations)} frames from {video_file}\\\\n\")\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Error: {e}\\\\n\")\n",
        "                continue\n",
        "\n",
        "        print(f\"✓ Dataset: {len(self.index_map)} frames from {len(self.video_tensors)} videos\\\\n\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.index_map)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        video_path, frame_idx, target = self.index_map[idx]\n",
        "        frame = self.video_tensors[video_path][frame_idx]\n",
        "        labels = target[\"moving\"].clone() + 1\n",
        "\n",
        "        return frame, {\n",
        "            \"boxes\": target[\"boxes\"],\n",
        "            \"labels\": labels\n",
        "        }\n",
        "\n",
        "\n",
        "def collate_fn(batch):\n",
        "    \"\"\"Custom collate function.\"\"\"\n",
        "    frames = [item[0] for item in batch]\n",
        "    targets = [item[1] for item in batch]\n",
        "    return frames, targets\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jc_wylzhAQUX",
        "outputId": "35cf58d1-6a08-4cff-9d26-3bd3c7d27ca5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7OC9VX9i_eRt"
      },
      "source": [
        "##  Neural Network Model & Training\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PKF-JiLo_eRt"
      },
      "outputs": [],
      "source": [
        "def get_model(num_classes, pretrained=True):\n",
        "    \"\"\"Create Faster R-CNN model.\"\"\"\n",
        "    model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=pretrained)\n",
        "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
        "    model.roi_heads.box_predictor = torchvision.models.detection.faster_rcnn.FastRCNNPredictor(\n",
        "        in_features, num_classes\n",
        "    )\n",
        "    return model\n",
        "\n",
        "\n",
        "def train_one_epoch(model, dataloader, optimizer, device):\n",
        "    \"\"\"Train for one epoch.\"\"\"\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "\n",
        "    for batch_idx, (images, targets) in enumerate(dataloader):\n",
        "        images = [img.to(device) for img in images]\n",
        "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
        "\n",
        "        loss_dict = model(images, targets)\n",
        "        losses = sum(loss for loss in loss_dict.values())\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        losses.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += losses.item()\n",
        "\n",
        "        if batch_idx % 10 == 0:\n",
        "            print(f\"  Batch {batch_idx}/{len(dataloader)} | Loss: {losses.item():.4f}\")\n",
        "\n",
        "    return total_loss / len(dataloader)\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def validate(model, dataloader, device):\n",
        "    \"\"\"Validate model.\"\"\"\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "\n",
        "    for images, targets in dataloader:\n",
        "        images = [img.to(device) for img in images]\n",
        "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
        "\n",
        "        loss_dict = model(images, targets)\n",
        "        losses = sum(loss for loss in loss_dict.values())\n",
        "        total_loss += losses.item()\n",
        "\n",
        "    return total_loss / len(dataloader)\n",
        "\n",
        "\n",
        "def train_model(train_dataset, val_dataset, num_classes=3, epochs=3, lr=5e-5, batch_size=2):\n",
        "    \"\"\"Train the model.\"\"\"\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(f\"Training on: {device}\\\\n\")\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
        "\n",
        "    model = get_model(num_classes, pretrained=True).to(device)\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        print(f\"Epoch {epoch+1}/{epochs}\")\n",
        "        print(\"-\" * 40)\n",
        "\n",
        "        train_loss = train_one_epoch(model, train_loader, optimizer, device)\n",
        "        val_loss = validate(model, val_loader, device)\n",
        "\n",
        "        print(f\"Train: {train_loss:.4f} | Val: {val_loss:.4f}\\\\n\")\n",
        "\n",
        "    print(\"✓ Training complete!\\\\n\")\n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LT5xJvA8_eRt"
      },
      "source": [
        "## Model Save/Load Functions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_-UY0boC_eRt"
      },
      "outputs": [],
      "source": [
        "def save_model(model, save_path=\"baseball_detector.pth\"):\n",
        "    \"\"\"Save trained model weights.\"\"\"\n",
        "    torch.save(model.state_dict(), save_path)\n",
        "    print(f\"✓ Model saved to '{save_path}'\")\n",
        "\n",
        "\n",
        "def load_model(weights_path, num_classes=3, device=None):\n",
        "    \"\"\"\n",
        "    IMPORT SCRIPT: Load trained model without retraining.\n",
        "    Use this function to evaluate the model.\n",
        "    \"\"\"\n",
        "    if device is None:\n",
        "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    print(f\"Loading model on: {device}\")\n",
        "\n",
        "    # Recreate architecture\n",
        "    model = get_model(num_classes, pretrained=False)\n",
        "\n",
        "    # Load saved weights\n",
        "    state_dict = torch.load(weights_path, map_location=device)\n",
        "    model.load_state_dict(state_dict)\n",
        "\n",
        "    # Set to evaluation mode\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    print(f\"✓ Model loaded from '{weights_path}'\\\\n\")\n",
        "    return model\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def predict(model, video_path, resize=(640, 480), confidence_threshold=0.5):\n",
        "    \"\"\"Run inference on a video.\"\"\"\n",
        "    device = next(model.parameters()).device\n",
        "    model.eval()\n",
        "\n",
        "    video_tensor = video_to_tensor(video_path, resize=resize)\n",
        "    predictions = []\n",
        "\n",
        "    for frame in video_tensor:\n",
        "        frame = frame.to(device)\n",
        "        pred = model([frame])[0]\n",
        "\n",
        "        mask = pred['scores'] > confidence_threshold\n",
        "        predictions.append({\n",
        "            'boxes': pred['boxes'][mask].cpu(),\n",
        "            'labels': pred['labels'][mask].cpu(),\n",
        "            'scores': pred['scores'][mask].cpu()\n",
        "        })\n",
        "\n",
        "    return predictions\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pwd9KvXF_eRu"
      },
      "source": [
        "## 4. Run Training\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "MoWcvv6G_eRu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4f401fb7-58dd-41d6-9978-f7cd61521732"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==================================================\n",
            "LOADING DATASET\n",
            "==================================================\\n\n",
            "Found 12 video files\\n\n",
            "Loading: IMG_7942_dusty.mov\n",
            "  -> 57 frames loaded\\n\n",
            "Indexed 57 frames from IMG_7942_dusty.mov\\n\n",
            "Loading: IMG_7997_khem.mov\n",
            "  -> 37 frames loaded\\n\n",
            "Indexed 37 frames from IMG_7997_khem.mov\\n\n",
            "Loading: IMG_7919_dusty.mov\n",
            "  -> 50 frames loaded\\n\n",
            "Indexed 50 frames from IMG_7919_dusty.mov\\n\n",
            "Loading: IMG_7998_khem.mov\n",
            "  -> 55 frames loaded\\n\n",
            "Indexed 55 frames from IMG_7998_khem.mov\\n\n",
            "Loading: IMG_9435_hugo.mov\n",
            "  -> 66 frames loaded\\n\n",
            "Indexed 66 frames from IMG_9435_hugo.mov\\n\n",
            "Loading: IMG_7943_khem.mov\n",
            "  -> 51 frames loaded\\n\n",
            "Indexed 54 frames from IMG_7943_khem.mov\\n\n",
            "Loading: IMG_7917_dusty.mov\n",
            "  -> 60 frames loaded\\n\n",
            "Indexed 60 frames from IMG_7917_dusty.mov\\n\n",
            "Loading: IMG_7918_dusty.mov\n",
            "  -> 76 frames loaded\\n\n",
            "Indexed 76 frames from IMG_7918_dusty.mov\\n\n",
            "Loading: dusty_1.mov\n",
            "  -> 76 frames loaded\\n\n",
            "Indexed 76 frames from dusty_1.mov\\n\n",
            "Loading: IMG_9197_hugo.mov\n",
            "  -> 70 frames loaded\\n\n",
            "Indexed 70 frames from IMG_9197_hugo.mov\\n\n",
            "Loading: IMG_9199_hugo.mov\n",
            "  -> 81 frames loaded\\n\n",
            "Indexed 81 frames from IMG_9199_hugo.mov\\n\n",
            "Loading: IMG_9609_dusty.mov\n",
            "  -> 102 frames loaded\\n\n",
            "Indexed 102 frames from IMG_9609_dusty.mov\\n\n",
            "✓ Dataset: 781 frames from 12 videos\\n\n",
            "Train: 624 samples\n",
            "Val: 157 samples\\n\n",
            "==================================================\n",
            "TRAINING\n",
            "==================================================\n",
            "Training on: cuda\\n\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=FasterRCNN_ResNet50_FPN_Weights.COCO_V1`. You can also use `weights=FasterRCNN_ResNet50_FPN_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/fasterrcnn_resnet50_fpn_coco-258fb6c6.pth\" to /root/.cache/torch/hub/checkpoints/fasterrcnn_resnet50_fpn_coco-258fb6c6.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 160M/160M [00:00<00:00, 177MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3\n",
            "----------------------------------------\n",
            "  Batch 0/312 | Loss: 115.9895\n",
            "  Batch 10/312 | Loss: 60.8186\n",
            "  Batch 20/312 | Loss: 84.5531\n",
            "  Batch 30/312 | Loss: 54.7157\n",
            "  Batch 40/312 | Loss: 30.7799\n",
            "  Batch 50/312 | Loss: 58.1665\n",
            "  Batch 60/312 | Loss: 57.9091\n",
            "  Batch 70/312 | Loss: 24.9072\n",
            "  Batch 80/312 | Loss: 29.5750\n",
            "  Batch 90/312 | Loss: 80.0623\n",
            "  Batch 100/312 | Loss: 17.1348\n",
            "  Batch 110/312 | Loss: 21.9460\n",
            "  Batch 120/312 | Loss: 29.8280\n",
            "  Batch 130/312 | Loss: 29.7437\n",
            "  Batch 140/312 | Loss: 26.9574\n",
            "  Batch 150/312 | Loss: 40.5491\n",
            "  Batch 160/312 | Loss: 26.0527\n",
            "  Batch 170/312 | Loss: 26.7784\n",
            "  Batch 180/312 | Loss: 27.2215\n",
            "  Batch 190/312 | Loss: 24.4175\n",
            "  Batch 200/312 | Loss: 22.9246\n",
            "  Batch 210/312 | Loss: 59.6585\n",
            "  Batch 220/312 | Loss: 28.3562\n",
            "  Batch 230/312 | Loss: 24.9417\n",
            "  Batch 240/312 | Loss: 38.6133\n",
            "  Batch 250/312 | Loss: 22.1172\n",
            "  Batch 260/312 | Loss: 21.5881\n",
            "  Batch 270/312 | Loss: 74.0798\n",
            "  Batch 280/312 | Loss: 39.3794\n",
            "  Batch 290/312 | Loss: 42.4798\n",
            "  Batch 300/312 | Loss: 20.9144\n",
            "  Batch 310/312 | Loss: 27.5209\n",
            "Train: 42.1604 | Val: 33.4740\\n\n",
            "Epoch 2/3\n",
            "----------------------------------------\n",
            "  Batch 0/312 | Loss: 39.4244\n",
            "  Batch 10/312 | Loss: 31.2400\n",
            "  Batch 20/312 | Loss: 33.5742\n",
            "  Batch 30/312 | Loss: 25.3662\n",
            "  Batch 40/312 | Loss: 75.4998\n",
            "  Batch 50/312 | Loss: 77.7304\n",
            "  Batch 60/312 | Loss: 26.1115\n",
            "  Batch 70/312 | Loss: 17.4246\n",
            "  Batch 80/312 | Loss: 71.8322\n",
            "  Batch 90/312 | Loss: 19.2559\n",
            "  Batch 100/312 | Loss: 84.4942\n",
            "  Batch 110/312 | Loss: 40.5514\n",
            "  Batch 120/312 | Loss: 28.8451\n",
            "  Batch 130/312 | Loss: 22.6982\n",
            "  Batch 140/312 | Loss: 20.5372\n",
            "  Batch 150/312 | Loss: 71.9264\n",
            "  Batch 160/312 | Loss: 25.2859\n",
            "  Batch 170/312 | Loss: 19.6966\n",
            "  Batch 180/312 | Loss: 26.4050\n",
            "  Batch 190/312 | Loss: 59.8913\n",
            "  Batch 200/312 | Loss: 65.0921\n",
            "  Batch 210/312 | Loss: 20.1705\n",
            "  Batch 220/312 | Loss: 10.2197\n",
            "  Batch 230/312 | Loss: 18.8409\n",
            "  Batch 240/312 | Loss: 26.8701\n",
            "  Batch 250/312 | Loss: 18.0005\n",
            "  Batch 260/312 | Loss: 46.6905\n",
            "  Batch 270/312 | Loss: 27.8822\n",
            "  Batch 280/312 | Loss: 37.9804\n",
            "  Batch 290/312 | Loss: 31.1594\n",
            "  Batch 300/312 | Loss: 17.7363\n",
            "  Batch 310/312 | Loss: 52.7096\n",
            "Train: 33.7247 | Val: 36.2720\\n\n",
            "Epoch 3/3\n",
            "----------------------------------------\n",
            "  Batch 0/312 | Loss: 43.3285\n",
            "  Batch 10/312 | Loss: 15.7209\n",
            "  Batch 20/312 | Loss: 20.1504\n",
            "  Batch 30/312 | Loss: 15.4295\n",
            "  Batch 40/312 | Loss: 58.1868\n",
            "  Batch 50/312 | Loss: 31.7936\n",
            "  Batch 60/312 | Loss: 27.2055\n",
            "  Batch 70/312 | Loss: 31.2630\n",
            "  Batch 80/312 | Loss: 41.6195\n",
            "  Batch 90/312 | Loss: 16.7081\n",
            "  Batch 100/312 | Loss: 72.9029\n",
            "  Batch 110/312 | Loss: 83.1434\n",
            "  Batch 120/312 | Loss: 16.3724\n",
            "  Batch 130/312 | Loss: 42.0546\n",
            "  Batch 140/312 | Loss: 17.2709\n",
            "  Batch 150/312 | Loss: 40.6635\n",
            "  Batch 160/312 | Loss: 7.4074\n",
            "  Batch 170/312 | Loss: 23.1630\n",
            "  Batch 180/312 | Loss: 31.0009\n",
            "  Batch 190/312 | Loss: 20.4378\n",
            "  Batch 200/312 | Loss: 27.3168\n",
            "  Batch 210/312 | Loss: 11.3762\n",
            "  Batch 220/312 | Loss: 14.8137\n",
            "  Batch 230/312 | Loss: 17.3948\n",
            "  Batch 240/312 | Loss: 13.6346\n",
            "  Batch 250/312 | Loss: 53.1741\n",
            "  Batch 260/312 | Loss: 10.5686\n",
            "  Batch 270/312 | Loss: 67.3395\n",
            "  Batch 280/312 | Loss: 15.0772\n",
            "  Batch 290/312 | Loss: 16.4931\n",
            "  Batch 300/312 | Loss: 22.3997\n",
            "  Batch 310/312 | Loss: 28.0740\n",
            "Train: 31.6493 | Val: 33.7052\\n\n",
            "✓ Training complete!\\n\n",
            "==================================================\n",
            "SAVING\n",
            "==================================================\\n\n",
            "✓ Model saved to 'baseball_detector.pth'\n",
            "\\n✓ DONE!\n"
          ]
        }
      ],
      "source": [
        "# Set paths to your data\n",
        "VIDEO_DIR = \"/content/sample_data/OneDrive_2_11-12-2025\"\n",
        "XML_DIR = \"/content/sample_data/OneDrive_1_11-12-2025/\"\n",
        "\n",
        "# Load dataset\n",
        "print(\"=\" * 50)\n",
        "print(\"LOADING DATASET\")\n",
        "print(\"=\" * 50 + \"\\\\n\")\n",
        "\n",
        "dataset = BaseballVideoDataset(\n",
        "    video_dir=VIDEO_DIR,\n",
        "    xml_dir=XML_DIR,\n",
        "    resize=(640, 480),\n",
        "    frame_skip=1\n",
        ")\n",
        "\n",
        "# Split train/validation (80/20)\n",
        "n = len(dataset)\n",
        "train_size = int(0.8 * n)\n",
        "val_size = n - train_size\n",
        "train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
        "\n",
        "print(f\"Train: {train_size} samples\")\n",
        "print(f\"Val: {val_size} samples\\\\n\")\n",
        "\n",
        "# Train model\n",
        "print(\"=\" * 50)\n",
        "print(\"TRAINING\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "model = train_model(\n",
        "    train_dataset,\n",
        "    val_dataset,\n",
        "    num_classes=3,  # background, not-moving, moving\n",
        "    epochs=3,\n",
        "    lr=5e-5,\n",
        "    batch_size=2\n",
        ")\n",
        "\n",
        "# Save model\n",
        "print(\"=\" * 50)\n",
        "print(\"SAVING\")\n",
        "print(\"=\" * 50 + \"\\\\n\")\n",
        "\n",
        "save_model(model, \"baseball_detector.pth\")\n",
        "\n",
        "print(\"\\\\n✓ DONE!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tg31bQr9_eRu"
      },
      "source": [
        "## Evalution\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "hM-o4p73_eRu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "76243063-e6ba-4be6-e4ec-a6ab6838419d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading model on: cuda\n",
            "✓ Model loaded from '/content/baseball_detector.pth'\\n\n",
            "Loading: dusty_1.mov\n",
            "  -> 76 frames loaded\\n\n",
            "✓ Predicted 76 frames\n",
            "Frame 0 predictions: 0 detections\n",
            "Frame 1 predictions: 1 detections\n",
            "Frame 2 predictions: 0 detections\n",
            "Frame 3 predictions: 0 detections\n",
            "Frame 4 predictions: 0 detections\n",
            "Frame 5 predictions: 0 detections\n",
            "Frame 6 predictions: 1 detections\n",
            "Frame 7 predictions: 0 detections\n",
            "Frame 8 predictions: 0 detections\n",
            "Frame 9 predictions: 1 detections\n",
            "Frame 10 predictions: 1 detections\n",
            "Frame 11 predictions: 0 detections\n",
            "Frame 12 predictions: 1 detections\n",
            "Frame 13 predictions: 1 detections\n",
            "Frame 14 predictions: 2 detections\n",
            "Frame 15 predictions: 2 detections\n",
            "Frame 16 predictions: 0 detections\n",
            "Frame 17 predictions: 0 detections\n",
            "Frame 18 predictions: 3 detections\n",
            "Frame 19 predictions: 2 detections\n",
            "Frame 20 predictions: 1 detections\n",
            "Frame 21 predictions: 2 detections\n",
            "Frame 22 predictions: 0 detections\n",
            "Frame 23 predictions: 1 detections\n",
            "Frame 24 predictions: 0 detections\n",
            "Frame 25 predictions: 0 detections\n",
            "Frame 26 predictions: 0 detections\n",
            "Frame 27 predictions: 0 detections\n",
            "Frame 28 predictions: 0 detections\n",
            "Frame 29 predictions: 0 detections\n",
            "Frame 30 predictions: 0 detections\n",
            "Frame 31 predictions: 0 detections\n",
            "Frame 32 predictions: 2 detections\n",
            "Frame 33 predictions: 1 detections\n",
            "Frame 34 predictions: 0 detections\n",
            "Frame 35 predictions: 0 detections\n",
            "Frame 36 predictions: 1 detections\n",
            "Frame 37 predictions: 1 detections\n",
            "Frame 38 predictions: 0 detections\n",
            "Frame 39 predictions: 1 detections\n",
            "Frame 40 predictions: 1 detections\n",
            "Frame 41 predictions: 2 detections\n",
            "Frame 42 predictions: 1 detections\n",
            "Frame 43 predictions: 1 detections\n",
            "Frame 44 predictions: 2 detections\n",
            "Frame 45 predictions: 2 detections\n",
            "Frame 46 predictions: 1 detections\n",
            "Frame 47 predictions: 1 detections\n",
            "Frame 48 predictions: 1 detections\n",
            "Frame 49 predictions: 1 detections\n",
            "Frame 50 predictions: 1 detections\n",
            "Frame 51 predictions: 0 detections\n",
            "Frame 52 predictions: 1 detections\n",
            "Frame 53 predictions: 0 detections\n",
            "Frame 54 predictions: 0 detections\n",
            "Frame 55 predictions: 4 detections\n",
            "Frame 56 predictions: 2 detections\n",
            "Frame 57 predictions: 1 detections\n",
            "Frame 58 predictions: 0 detections\n",
            "Frame 59 predictions: 0 detections\n",
            "Frame 60 predictions: 2 detections\n",
            "Frame 61 predictions: 1 detections\n",
            "Frame 62 predictions: 3 detections\n",
            "Frame 63 predictions: 1 detections\n",
            "Frame 64 predictions: 1 detections\n",
            "Frame 65 predictions: 1 detections\n",
            "Frame 66 predictions: 1 detections\n",
            "Frame 67 predictions: 2 detections\n",
            "Frame 68 predictions: 1 detections\n",
            "Frame 69 predictions: 1 detections\n",
            "Frame 70 predictions: 4 detections\n",
            "Frame 71 predictions: 2 detections\n",
            "Frame 72 predictions: 2 detections\n",
            "Frame 73 predictions: 2 detections\n",
            "Frame 74 predictions: 1 detections\n",
            "Frame 75 predictions: 0 detections\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Load the trained model\n",
        "loaded_model = load_model(\"/content/baseball_detector.pth\", num_classes=3)\n",
        "\n",
        "# Test on a video\n",
        "test_video = \"/content/sample_data/OneDrive_2_11-12-2025/dusty_1.mov\"\n",
        "predictions = predict(loaded_model, test_video, confidence_threshold=0.5)\n",
        "\n",
        "print(f\"✓ Predicted {len(predictions)} frames\")\n",
        "for i, frame_preds in enumerate(predictions):\n",
        "    print(f\"Frame {i} predictions: {len(frame_preds['boxes'])} detections\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.6"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}